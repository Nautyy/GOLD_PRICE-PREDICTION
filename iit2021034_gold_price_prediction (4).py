# -*- coding: utf-8 -*-
"""IIT2021034_Gold_Price_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ONr57HkBAo5YOvROD7w5Ri7V4vEFKLq3

Importing the Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn import metrics
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow.keras import layers,models
from tensorflow.keras.models import Sequential
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsRegressor

# Assuming you have loaded your gold_data DataFrame
gold_data = pd.read_csv('/content/gld_price_data.csv')

# GAN parameters
latent_dim_gan = 100
output_shape_gan = gold_data.shape[1] - 1  # Adjust this based on the number of features
num_synthetic_samples_gan = 4000

# Define a simple GAN generator model
def build_generator(latent_dim, output_shape):
    model = Sequential()  # Use Sequential directly from tensorflow.keras.models
    model.add(layers.Dense(128, input_dim=latent_dim, activation='relu'))
    model.add(layers.Dense(256, activation='relu'))
    model.add(layers.Dense(output_shape, activation='linear'))  # Adjust activation based on your data
    return model

# Function to train the GAN generator
def train_gan_generator(generator, latent_dim, num_samples):
    noise = np.random.normal(0, 1, size=(num_samples, latent_dim))
    fake_samples = generator.predict(noise)
    return fake_samples

# Build and compile the GAN generator model
generator = build_generator(latent_dim_gan, output_shape_gan)
generator.compile(optimizer='adam', loss='mean_squared_error')  # Adjust loss based on your data

# Train the GAN generator
synthetic_data_gan = train_gan_generator(generator, latent_dim_gan, num_synthetic_samples_gan)

# Combine original and synthetic data
combined_data = pd.concat([gold_data.drop(['GLD'], axis=1), pd.DataFrame(synthetic_data_gan, columns=gold_data.columns[1:])], ignore_index=True)
combined_labels = pd.concat([gold_data['GLD'], pd.Series(np.zeros(num_synthetic_samples_gan))], ignore_index=True)


# Splitting into Training data and Test Data (use the combined data)
X_train_combined, X_test_combined, Y_train_combined, Y_test_combined = train_test_split(
    combined_data, combined_labels, test_size=0.2, random_state=2
)

# Combine original and synthetic data
combined_data = pd.concat([gold_data.drop('GLD', axis=1), pd.DataFrame(synthetic_data_gan, columns=gold_data.columns[1:])], ignore_index=True)
combined_labels = pd.concat([gold_data['GLD'], pd.Series(np.zeros(num_synthetic_samples_gan))], ignore_index=True)

# Print the number of samples before and after combining
print("Number of samples before combining:", len(gold_data))
print("Number of synthetic samples generated by GAN:", num_synthetic_samples_gan)
print("Number of samples after combining:", len(combined_data))

"""Data Collection and Processing"""

# print first 5 rows in the dataframe
combined_data.head()

# print last 5 rows of the dataframe
combined_data.tail()

# number of rows and columns
combined_data.shape

# getting some basic informations about the data
gold_data.info()

# checking the number of missing values
gold_data.isnull().sum()

"""# New Section"""

# getting the statistical measures of the data
gold_data.describe()

"""Correlation:
1. Positive Correlation
2. Negative Correlation
"""

correlation = gold_data.corr()

# constructing a heatmap to understand the correlatiom
plt.figure(figsize = (8,8))
sns.heatmap(correlation, cbar=True, square=True, fmt='.1f',annot=True, annot_kws={'size':8}, cmap='Blues')

# correlation values of GLD
print(correlation['GLD'])

# checking the distribution of the GLD Price
sns.distplot(gold_data['GLD'],color='green')

"""Splitting the Features and Target"""

X = gold_data.drop(['Date','GLD'],axis=1)
Y = gold_data['GLD']

print(X)

print(Y)

"""Splitting into Training data and Test Data"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=2)

from sklearn.metrics import mean_absolute_error, explained_variance_score, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Set a color palette with distinct colors
colors = sns.color_palette("husl", len(models))

# Train Linear Regression model
linear_regressor = LinearRegression()
linear_regressor.fit(X_train, Y_train)
linear_pred = linear_regressor.predict(X_test)

# Train Support Vector Regression (SVR) model
svr_regressor = SVR()
svr_regressor.fit(X_train, Y_train)
svr_pred = svr_regressor.predict(X_test)

# Train AdaBoost model
adaboost_regressor = AdaBoostRegressor()
adaboost_regressor.fit(X_train, Y_train)
adaboost_pred = adaboost_regressor.predict(X_test)

# Train k-Nearest Neighbors (KNN) model
knn_regressor = make_pipeline(StandardScaler(), KNeighborsRegressor())
knn_regressor.fit(X_train, Y_train)
knn_pred = knn_regressor.predict(X_test)

# Define a list of metrics
metrics_list = ['Mean Squared Error', 'Mean Absolute Error', 'R-squared', 'Explained Variance']

models = {
    'Random Forest Regressor': regressor,
    'Linear Regression': linear_regressor,
    'SVR': svr_regressor,
    'AdaBoost Regressor': adaboost_regressor,
    'KNN Regressor': knn_regressor
}

best_model_name = None
best_model_mse = float('inf')  # Initialize with a large value
best_model_r2 = -float('inf')  # Initialize with a small value

plt.figure(figsize=(15, 10))

for i, (name, model) in enumerate(models.items(), start=1):
    plt.subplot(2, 3, i)

    predictions = model.predict(X_test)
    mse = mean_squared_error(Y_test, predictions)
    mae = mean_absolute_error(Y_test, predictions)
    r2 = r2_score(Y_test, predictions)
    ev = explained_variance_score(Y_test, predictions)

    plt.plot(Y_test, predictions, label=f'{name}\nMSE: {mse:.2f}\nMAE: {mae:.2f}\nR2: {r2:.2f}\nEV: {ev:.2f}', linestyle='None', marker='o', color=colors[i-1])
    plt.title(f'Actual vs. Predicted Values ({name})')
    plt.xlabel('Actual Values')
    plt.ylabel('Predicted Values')
    plt.legend()

    # Update the best model if the current model performs better
    if mse < best_model_mse and r2 > best_model_r2:
        best_model_name = name
        best_model_mse = mse
        best_model_r2 = r2

plt.tight_layout()
plt.show()

# Print the best model
print(f"The best model is: {best_model_name}")
print(f"Best Model Mean Squared Error: {best_model_mse}")
print(f"Best Model R-squared: {best_model_r2}")

"""Model Training:
Random Forest Regressor
"""

regressor = RandomForestRegressor(n_estimators=100)

# training the model
regressor.fit(X_train,Y_train)

"""Model Evaluation"""

# prediction on Test Data
test_data_prediction = regressor.predict(X_test)

print(test_data_prediction)

# R squared error
error_score = metrics.r2_score(Y_test, test_data_prediction)
print("R squared error : ", error_score)

"""Compare the Actual Values and Predicted Values in a Plot"""

Y_test = list(Y_test)

plt.plot(Y_test, color='blue', label = 'Actual Value')
plt.plot(test_data_prediction, color='green', label='Predicted Value')
plt.title('Actual Price vs Predicted Price')
plt.xlabel('Number of values')
plt.ylabel('GLD Price')
plt.legend()
plt.show()

# Load the dataset
df = pd.read_csv('gld_price_data.csv')

# Convert the 'Date' column to datetime format
df['Date'] = pd.to_datetime(df['Date'], format='%m/%d/%Y')  # Adjust the date format

# Extract year and month from the 'Date' column
df['year'] = df['Date'].dt.year
df['month'] = df['Date'].dt.month

# Calculate the average price for each month
average_prices_by_month = df.groupby(['year', 'month'])['GLD'].mean().unstack()

# Identify the month with the lowest average price (considered as the best time to buy)
best_month_to_buy = average_prices_by_month.mean(axis=0).idxmin()
best_month_name = df[df['month'] == best_month_to_buy]['Date'].dt.strftime('%B').iloc[0]

print(f'The best month to buy gold is {best_month_name} ({best_month_to_buy})')

# Plotting the average gold prices by month
plt.figure(figsize=(14, 8))

# Plot year-wise trends
for year in average_prices_by_month.index:
    plt.plot(average_prices_by_month.columns, average_prices_by_month.loc[year], label=year)

plt.title('Year-wise Gold Price Trends')
plt.xlabel('Month')
plt.ylabel('Average Price ($)')
plt.xticks(range(1, 13), [month[:3] for month in df['Date'].dt.strftime('%B').unique()])  # Display abbreviated month names
plt.legend(title='Year', loc='upper left', bbox_to_anchor=(1, 1))
plt.grid(True)
plt.show()

# Plotting the average gold prices by month and best month to buy
plt.figure(figsize=(10, 6))
plt.plot(average_prices_by_month.columns, average_prices_by_month.mean(), marker='o', linestyle='-', color='b')
plt.title('Average Gold Prices by Month')
plt.xlabel('Month')
plt.ylabel('Average Price ($)')
plt.xticks(range(1, 13), [month[:3] for month in df['Date'].dt.strftime('%B').unique()])  # Display abbreviated month names
plt.grid(True)
plt.axvline(x=best_month_to_buy, color='r', linestyle='--', label=f'Best Month to Buy ({best_month_name})')
plt.legend()
plt.show()